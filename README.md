# **GPT COMPONENTS**
In this directory there contains a python module for position embedding 
which carries out the positiona encoding technology found in the paper
"Attention is All you Need" by Google.
It uses Tensorflow and keras as the framework and API to build this
particular module

There also lies a test_pos.py which verifies that the positional encoder works properly

The control flow of GPT-2(large) which is a Decoder-Only Transformer is given below:
